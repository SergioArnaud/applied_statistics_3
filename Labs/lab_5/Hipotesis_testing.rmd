---
title: "Ejercicios sobre hipótesis e intervalos de confianza"
author: "Jorge de la Vega"
date: "13 de septiembre de 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NULL, size="small",fig.height = 5)
```


## Construcción de intervalos simultáneos de confianza y elipses.

### Ejemplo 1. 

Los scores obtenidos por 87 estudiantes de colegios para tres materias son:
$X_1$ = ciencias sociales e historia
$X_2$ = verbal
$X_3$ = ciencia

```{r}
X <- read.table("https://piazza.com/class_profile/get_resource/jkraoi4gb2l77m/jm19n7ranu16d6",
           col.names = c("X1","X2","X3"))
n <- dim(X)[1]; p <- dim(X)[2] #dimensiones
# Estimadores
xbar <- colMeans(X)  
S <- var(X)
```

¿Estos datos son normales?

```{r}
pairs(X)
par(mfrow=c(1,3))
for(i in 1:3)qqnorm(X[,i])
```


Para obtener los intervalos de confianza simultáneos del 95% para $\mu_1$, $\mu_2$ y $\mu_3$, calculamos:
```{r}
c1 <- p*(n-1)/(n-p)*qf(.05,p,n-p,lower.tail = F)
for(i in 1:3)print(xbar[i] + c(-1,1)*sqrt(c1*diag(S)[i]/n ))
```

Usualmente, los intervalos simultáneos basados en la $T^2$ de Hotelling tienden a ser más anchos que los intervalos marginales porque toman en cuenta todas las posibles combinaciones lineales. Por ejemplo, si probamos $\mu_2-\mu_3$, usamos el mismo factor al 95% de confianza:

```{r}
a <- c(0,1,-1)  #contraste entre medias
(as.numeric(t(a)%*%xbar)) + c(-1,1)*sqrt(c1* as.numeric(t(a)%*%S%*%a)/n)
```

Ahora consideremos los **intervalos individuales** para los parámetros:

```{r}
c2 <- qt(.025,n-1,lower.tail = F)
for(i in 1:3) print(xbar[i] + c(-1,1)*c2*sqrt(S[i,i]/n))
```

### Método de Bonferroni para comparaciones múltiples

El método de Bonferroni pretende mejorar la estimación del intervalo, considerando que no todas las combinaciones lineales son relevantes o son de interés. Es un punto de vista intermedio entre los intervalos individuales y los intervaloes simultáneos que busca obtener intervalos más precisos. En muchos contextos se preferirán los intervalos bonferronizados a los simultáneos.

Este método se basa en la desigualdad de Bonferroni.

Ex-ante, se requiere hacer afirmaciones de $k$  combinaciones lineales con vectores de coeficientes $a_1,\ldots a_k$. Para cada una de estas combinaciones lineales, hay una región de confianza $C_i$ para $H_i$ y supongamos que $P(C_i)= 1-\alpha_i$. Entonces:
\[ P(\bigcap C_i) = 1 - P(\bigcup C_i^c)  \geq 1- \sum_{i=1}^kP(C_i^c) = 1- \sum_{i=1}^k(1-P(C_i)) = 1-\sum_{i=1}^k \alpha_k \]

Entonces se puede buscar controlar las $\alpha_i's$ y el margen de significancia para las pruebas importantes. Suponiendo que las $p$ medias valieran lo mismo, típicamente se puede hacer $\alpha_i=\alpha/p$:los intervalos _bonferronizados_ se obtienen tomando el percentil $\alpha/2p$ de una $t_{n-1}$:

```{r}
c3 <- qt(.025/p,n-1,lower.tail = F)
for(i in 1:3) print(xbar[i] + c(-1,1)*c3*sqrt(S[i,i]/n))
```

¿Qué tanto se reducen los intervalos bonferronizados respecto a los simultaneos? Hacer una tabla comparando los valores con $p$ variables, $n$ observaciones, y tomando $1-\alpha=0.95$

## Ejemplo 2: Aplicación a Control de Calidad

En los procesos de producción se tienen que controlar los parámetros para que no se afecte la calidad del producto o servicio.

Una _gráfica de control_, que consiste de una gráfica de los datos medidos en función del tiempo, sirve para identificar y visualizar ocurrencias de causas especiales de variación, como puede ser fallas de componentes, etc. Estos valores usualmente rebasan _límites de control_.

Los siguientes datos corresponden a mediciones de 4 variables medidas en intervalos de 5 segundos de una soldadora en una lńea de producción de autos. Las variables son:
$Y_1=$ voltaje (volts)
$Y_2=$ Corriente (amperes)
$Y_3=$ velocidad de alimentación (pulgadas/minuto)
$Y_4=$ Flujo de gas inerte (pies cúblicos/ minuto)

```{r}
Y <- read.table("https://piazza.com/class_profile/get_resource/jkraoi4gb2l77m/jm1fvrje36twi",
                    col.names = paste0("Y",1:4))
dim(Y); n <-dim(Y)[1]; p <- dim(Y)[2]
head(Y)
# Construcción de la gráfica de control
# Para cada una de las variables se crea la gráfica de los puntos con la media y dos lineas
# Upper control limit (UCL) mean(x) + 3*sd(x)
# Lower control limit (LCL) mean(x) - 3*sd(x)
par(mfrow=c(2,2))
for(i in 1:p){
  plot(Y[,i], main = names(Y)[i], ylim = c(min(Y[,i]),max(Y[,i])) + c(-1,1)*3*sd(Y[,i]),
       xlab="índice", ylab=names(Y)[i])
  abline(h=mean(Y[,i])+c(-1,0,1)*3*sd(Y[,i]),col="red",lwd=2)
}
```

Hay dos versiones multivariadas de una gráfica de control, que tome en cuenta las correlaciones entre las diferentes variables:

1. Elipse de control
2. Gráfica $T^2$

La elipse de control es básicamente la que se obtiene con los estimadores máximo-verosímiles de dos variables a la vez:
```{r}
library(ellipse)
indices <- combn(1:p,2,FUN=list,simplify = T) #lista de combinación de índices
variables <- combn(1:p,2,FUN=function(x)paste0("Y",x)) #matriz de nombres de la combinación
par(mfrow=c(2,3))
for(i in 1:choose(p,2)){
  ind <- indices[[i]]
  plot(Y[,ind], xlim = mean(Y[,ind[1]])+c(-4,4)*sd(Y[,ind[1]]),ylim = mean(Y[,ind[2]])+c(-4,4)*sd(Y[,ind[2]]))
  lines(ellipse(var(Y[,ind]),centre=colMeans(Y[,ind]),level=0.99),col="red",type="l")
}
```

La gráfica $T^2$ calcula para cada observación el valor de la estadística $T^2$ y la compara contra el límite superior UCL

```{r}
Ti2 <- numeric()
for(i in 1:n) Ti2[i] = as.numeric(as.matrix(Y[i,] - colMeans(Y)) %*% solve(var(Y)) %*% t(Y[i,] - colMeans(Y)))
plot(Ti2,ylim=c(0,1.4*max(Ti2)),pch=16)
abline(h=qchisq(.95,p),col="yellow",lwd=3) #UCL
abline(h=qchisq(.99,p),col="red",lwd=3) #UCL
```

